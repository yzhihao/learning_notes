# 斯坦福机器学习3

#第九讲

## 偏差/方差权衡

### 非正式的定义模型的偏差（bias）
即模型的预测值与样本实际值之间的差距，偏差越大则约偏离真实数据，即使拟合的训练集很大（如无限的训练集）

### 方差：
就是预测值的变化范围或离散程度。方差越大，数据分布越分散。可以用泛化误差来理解，泛化误差很大。

## 两个引理
1. 联合界引理：用集合知识来理解，就是并集肯定小于等于两个集合的和
2. 霍夫丁不等式引理：集合现实抛硬币，假设有一枚不均匀的硬币，掷这枚硬币面朝上的概率为ϕ，如果我们掷m次硬币，然后计算面朝上的比例，那么掷的次数越多，则这个比例作为参数ϕ估计的可信度越高。，注意那个式子代表的意义。**表示实际值和理论值之间的差距**

## 选取分类器
1. 不过我们不仅仅想保证这个特定的hi下ε(hi)ε(hi)与ε^(hi)ε^(hi)（很大概率）非常接近，我们想要证明这对所有的h∈H同时成立。
2. 也就是说，即使在假设类中增加很多假设，所需的样本也不会增加多少

## 关于H
* 对于本讲要证明的结论来说，我们不再把学习算法当做是一组参数的选取过程，而应该把它当做是选择一个函数的过程。定义学假设类（hypothesis class）HH是所有分类器的集合，而我们的学习算法会从中选择分类器。

* 因此H只是k个从X映射到{0,1}的函数组成的集合，而经验风险最小化将从这kk个函数中选择训练误差最小的一个。

* 整个过程说明，如果我们切换到一个更大的假设类中，那么我们得到一个使泛化误差更小的假设函数的希望就会增加，但代价是找到的函数不能精确拟合训练集的概率也会增加。我们可以非正式的将第一项对应为偏差，而将第二项对应为方差

##经验风险最小化
。经验风险最小化通常被认为是最基础的学习算法。（ERM本身是一个非凸优化问题，诸如逻辑回归和支持向量机等算法也可以被看做是一种凸性近似的经验风险最小化算法。）

#第十讲
1. 所需训练样本的数量与模型参数的个数大约最多呈线性关系。
2. 到目前为止，这些结论并不能说明一个使用经验风险最小化的算法的价值。因此，虽然多数尝试最小化或估计训练误差的判别学习算法对参数个数d为线性复杂度，但这个结论并不是对所有判别学习算法有效。对很多非经验风险最小化使用良好的假设保证仍是热门研究方向。就是用


## H 分散
1. 存在任意h满足公式
2. 如果HH可以分散任意大小的集合，则有VC(H)=∞

2. 从VC维的定义有，为了证明VC(H)VC(H)至少是d，那么我们只需要找到一个大小为d的集合可以被H散列即可

综合上面的各结论有，（对于一个尝试最小化训练误差）算法所需的训练样本的数量通常与假设类HH参数的个数呈大致上的线性关系。这也表明了样本复杂度的上下界都是由VCVC维给出的。

## 正则化及模型选择

### 支持向量机算法与VC维
1. 前提：假设类将只包含这些间隔较大的判别边界。
2. 如果给定SVM一个需要满足的间隔，SVM将自动找到一个具有较小VCVC维的假设类，算法并不会发生过拟合的现象
3. 
关于这个结论的证明我们就不展开介绍了。通过这个结论我们可以知道，VC维数的取值范围与输入特征的维数是没有关系的。也就是说，即使输入x来自无限维空间，但只要我们令分类器必须满足一个较大的间隔时，VC维实际上是被限制在一个比较小的数字上。所以，如果给定SVM一个需要满足的间隔，SVM将自动找到一个具有较小VC维的假设类，算法并不会发生过拟合的现象




## 交叉验证
1. 用S训练每一个Mi，从而得到相应的假设函数hi；
2. 选择训练误差最小的假设函数。

但整个k重计算过程的代价将比原始的保留交叉验证大很多，因为我们需要训练每个模型k次

>是不是重交叉验证就看是不是要在每次训练中都要去验证

## 特征选择
封装特征选择
过滤特征选择

#第十一讲

## 贝叶斯统计与正则化


### 频率学派-（ch11）
频率学派（frequentist）的观点看，θθ是一个未知的但却是常量的值。对于频率派来说，θθ并不是随机变量，它是一个参数，只是恰巧不知道它的值而已，而我们的任务就是通过使用一些统计步骤（如最大似然法）来估计这个参数的值。

### 贝叶斯学派
贝叶斯学派的观点看问题，则会找到另一种估计参数的方法——他们将θθ看做是一个随机变量，其值未知。于是，我们可以假设一个关于θθ的先验分布（prior distribution）p(θ)p(θ)，用来表示我们“相信”参数取值的不确定性

## 感知及大间隔分类器
在线学习（online learning）算法：是一个即使在在学习的过程中也要做出预测的算法。

## 机器学习应用的一些建议

###  调试学习算法


