# 随机森林-AdaBoost

## 随机森林的定义
随机森林，指的是利用多棵树对样本进行训练并预测的一种分类器。该分类器最早由Leo Breiman和Adele Cutler提出，并被注册成了商标。简单来说，随机森林就是由多棵CART（Classification And Regression Tree）构成的。对于每棵树，它们使用的训练集是从总的训练集中有放回采样出来的，这意味着，总的训练集中的有些样本可能多次出现在一棵树的训练集中，也可能从未出现在一棵树的训练集中。在训练每棵树的节点时，使用的特征是从所有特征中按照一定比例随机地无放回的抽取的，根据Leo Breiman的建议，假设总的特征数量为M，这个比例可以是sqrt(M),1/2sqrt(M),2sqrt(M)。

## 步骤
因此，随机森林的训练过程可以总结如下：

(1)给定训练集S，测试集T，特征维数F。确定参数：使用到的CART的数量t，每棵树的深度d，每个节点使用到的特征数量f，终止条件：节点上最少样本数s，节点上最少的信息增益m
对于第1-t棵树，i=1-t：
(2)从S中有放回的抽取大小和S一样的训练集S(i)，作为根节点的样本，从根节点开始训练
(3)如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的预测输出为当前节点样本集合中数量最多的那一类c(j)，概率p为c(j)占当前样本集的比例；如果是回归问题，预测输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。如果当前节点没有达到终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的一维特征k及其阈值th，当前节点上样本第k维特征小于th的样本被划分到左节点，其余的被划分到右节点。继续训练其他节点。有关分类效果的评判标准在后面会讲。
(4)重复(2)(3)直到所有节点都训练过了或者被标记为叶子节点。
(5)重复(2),(3),(4)直到所有CART都被训练过。
利用随机森林的预测过程如下：
对于第1-t棵树，i=1-t：
(1)从当前树的根节点开始，根据当前节点的阈值th，判断是进入左节点(<th)还是进入右节点(>=th)，直到到达，某个叶子节点，并输出预测值。
(2)重复执行(1)直到所有t棵树都输出了预测值。如果是分类问题，则输出为所有树中预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值。


## 用途
1. 随机森林的最佳使用实例之一是特征选择（feature selection）。尝试许多决策树变量（variations）带来的副产品之一是，你可以检验每棵树中哪个变量最相关/无关。
2. 随机森林也很擅长分类任务。它能用于对具有多个可能值的类别进行预测，也能被校准来输出概率。需要注意的是过拟合（overfitting）。随机森林可能容易过拟合，尤其是使用相对小型的数据集时。如果你的模型在我们的测试集中表现“太好”，就应该怀疑过拟合了。
3. 　我发现随机森林——不像其他算法——在学习分类变量或分类变量和真实变量的结合时真的很有效。高基数的分类变量处理起来很棘手，因此随机森林会大有帮助。

http://blog.csdn.net/lo_cima/article/details/50533010

http://blog.csdn.net/haimengao/article/details/49615955

http://blog.csdn.net/a_31415926/article/details/50192917

http://blog.csdn.net/databatman/article/details/49406727