# 斯坦福大学机器学习1

# 第二，三讲

## 线性回归
一种分类方式，公式就是hθ(x)=θ0+θ1x1+θ2x2，非常简单

### 随机梯度下降
*　值得注意的是，随机梯度下降可能永远都不会收敛于最小值点，参数θ将在J(θ)最小值附近持续摆动。不过，在实践中，最小值附近的解通常都足够接近最小值。另外，在随机梯度下降的实际操作中，随着迭代步骤的进行，我们会慢慢减小α的值至0，这样也可以保证参数收敛于全局最小值，而不是在其附近持续摆动

**问：如果一开始选的点就是最低点，那随机梯度下降将会怎么工作**
**答：会保持选的点就是最佳参数，这是因为那个偏导数为0（联系2次函数）**

**问：梯度下降的公式是怎么来的**
**答：理解在2次函数的时候来理解，就是参数减去导数的值，然后更新参数，来自：coursera机器学习**

**问：批量和随机梯度的本质区别是什么**
**答：就是成本函数的不同，批量是计算把所有的数据都误差和，然后求导得，随机的成本函数是随机拿一条数据来作为误差，然后求导**

## 局部加权回归
线性回归的推广，就是在求参的时候不同，可以得到多个”子模型“，但模型同样是线性的。



## 缩减特征
* 为了使得随机梯度下降工作的更好，一班要把轮廓变为差不多圆形，就要用到特征缩放。
* 方法：就是(x-（均值）)/（最大到最小的范围）
* [具体看课件](https://www.coursera.org/learn/machine-learning/supplement/CTA0D/gradient-descent-in-practice-i-feature-scaling)
* si is the range of values (max - min), or si is the standard deviation.除数可以是标准差，也可以是极差


### 关于参数变化率a
* If α is too small: slow convergence.
* If α is too large: ￼may not decrease on every iteration and thus may not converge.

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/rC2jGKgvEeamBAoLccicqA_ec9e40a58588382f5b6df60637b69470_Screenshot-2016-11-11-08.55.21.png?expiry=1482883200000&hmac=S4Rrw7PMlQ736AKqVhRaMHC58LkGZnDaDrwIXCRlv-c)

### 正规方程组发

**问：这个方法和坐标下降法的本质区别是什么？**
****

### 方向导数
自变量是多个标量，或者理解成一个多维的向量。那么，函数随自变量的变化怎么刻画呢？一个方法，就是衡量函数在给定方向上的变化率，这就是方向导数。方向导数的特例，就是函数随各个自变量（标量）的变化率，即函数的偏导数，也就是函数沿各个坐标轴正方向的方向导数。

[理解方向导数和](https://www.zhihu.com/question/36301367)

### 梯度的理解
* 在个偏导数的方向就是其梯度的方向，这也是变化最快的方向。




看高数下册
[详数学推导](http://blog.csdn.net/lotus___/article/details/20546259#comments)
最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。

## 最小二乘和高斯-牛顿法
最小二乘法的目标：求误差的最小平方和，对应有两种：线性和非线性。线性最小二乘的解是closed-form即，而非线性最小二乘没有closed-form，通常用迭代法求解。

迭代法，即在每一步update未知量逐渐逼近解，可以用于各种各样的问题（包括最小二乘），比如求的不是误差的最小平方和而是最小立方和。

梯度下降是迭代法的一种，可以用于求解最小二乘问题（线性和非线性都可以）。高斯-牛顿法是另一种经常用于求解非线性最小二乘的迭代法（一定程度上可视为标准非线性最小二乘求解方法）。

还有一种叫做Levenberg-Marquardt的迭代法用于求解非线性最小二乘问题，就结合了梯度下降和高斯-牛顿法。

所以如果把最小二乘看做是优化问题的话，那么梯度下降是求解方法的一种，是求解线性最小二乘的一种，高斯-牛顿法和Levenberg-Marquardt则能用于求解非线性最小二乘。

>看出牛顿法和梯度下降都是求最小二乘的一种方式，最终目的就是求参数

# 第三讲

## 逻辑回归
* 决策边界的确定，这个主要是和x的多少此方有关，详细看[Decision Boundary](https://www.coursera.org/learn/machine-learning/supplement/N8qsm/decision-boundary)

### 逻辑回归和线性回归
* 逻辑回归就是在线性回归上加一个函数。
* 逻辑函数不不能直接向线性回归那样定义代价函数，因为那样不是凸优化的函数，有局部最优解

### Cost Function
*　用log函数来

### 过拟合的解决方法
*  Reduce the number of features:
    1. Reduce the number of features:Manually select which features to keep.
    2. Use a model selection algorithm (studied later in the course).
* Regularization(正则化)
    1. Keep all the features, but reduce the magnitude of parameters θj.
    2. Regularization works well when we have a lot of slightly useful features.


##  正则化

1. 修改损失函数，具体看[课件](https://www.coursera.org/learn/machine-learning/supplement/1tJlY/cost-function)

# 第4讲

## 一般线性模型

### 指数族
所有分布都可以写成指数族的分布一般式。

目的：由问题，用指数族分布一般式推出x->y的关系式。注意是什么和什么的关系

多项分布（multinomial），我们将在后面介绍，如同伯努利分布对有两个结果的事件建模一样，多项分布对有KK个结果的事件建模；

泊松分布（Poisson），用于计数数据建模，比如样本中放射性元素衰变的书目、某网站访客的数量、商店顾客的数量；

伽马分布和指数分布（the gamma and the exponential），用于非负的连续随机变量建模，如处理时间间隔，或是处理在等公交时发生的“下一辆车什么时候到”的问题；

贝塔分布和狄利克雷分布（the beta and the Dirichlet），通常用于对分数进行建模，它是对概率分布进行建模的概率分布；
Wishart分布，这是协方差矩阵的分布。


一般的，对于想要预测的关于xx的随机变量yy的分类或回归问题，我们为了推导出关于问题的一般线性模型。


#### 求特定线性模型的步骤：
1. 写出y∣x;θ符合的分布公式->还原出指数族分布一般式->找到E[y∣x;θ]和自然参数的关系->由自然参数和输入特征值x是线性关系η=θTx->得到模型公式

#### 符合条件
1. 一般的，对于想要预测的关于xx的随机变量yy的分类或回归问题，我们为了推导出关于问题的一般线性模型，模型需要遵守以下三条关于给定xx下yy的条件概率的假设：

>自然参数η（通常是实数）和输入特征值x是线性关系,η=θTx,所以求参的时候用梯度下降也合适。

一般逻辑回归
，Softmax回归。


#第5讲

## 判别学习算法和生成学习算法

判别学习算法：(DLA通过建立输入空间X与输出标注{1, 0}间的映射关系学习得到p(y|x))，直接建立一个模型，从x——>y

生成学习算法：(GLA首先确定p(x|y)和p(y)，由贝叶斯准则得到后验分布![](http://images.cnitblog.com/blog/405927/201412/050156335615212.png),通过最大后验准则进行预测，![](http://images.cnitblog.com/blog/405927/201412/050156342172611.png)
对y进行分类，建立多个模型，并求其概率。


## 生成学习算法
定义：我们观察大象的样本，然后建立一个描述大象的模型；接着，观察小狗样本，建立另一个描述小狗的模型；最后，当需要判断一个新的动物样本是大象还是小狗时，先用样本和大象模型做比对，再用样本和小狗模型做比对，然后看新样本究竟是更像大象还是更像小狗。


目的：一类建模方式，包括高斯判别分析模型和朴素贝叶斯算法
>看chapter05.ipynb

### 高斯判别分析模型
目的：一种建模方式：它的输入特征x为一系列连续取值的随机变量，这时我们就可以使用高斯判别分析（GDA: Gaussian Discriminant Analysis）模型了，使用多元正态分布对p(x∣y)建模。

求模型参数：为了选择适当的参数使l能够取到最大值，我们需要计算下列参数的最大似然估计：
>看chapter05.ipynb


### 朴素贝叶斯算法和高斯高斯判别分析的不同
高斯判别分析中，输入特征x是一个连续的实向量。现在我们来看另一种学习算法，其输入特征x是离散的。

模型公式：求得y=1的概率 p(y=1∣x)=p(x∣y=1)*p(y=1)/p(y=1)，求得y=0的概率同理

目的：建模方式
>看chapter05.ipynb

### 高斯高斯判别分析和logistic回归的关系
p(x|y)属于高斯分布，计算p(y|x)时，几乎能得到和logistic回归中使用的sigmoid函数一样的函数。但实际上还是存在本质区别的。（逆推并不成立，因为高斯的假设更多）
注意看什么时候用那个分类方法。

### 朴素贝叶斯

>**注意看《统计学习方法》p50例题**

**问：为什么在贝叶斯中要先求p(x|y)和p(y),而不是直接求p(y|x)**

**问：为什么可以去掉贝叶斯的分母？**
**答：因为那个表示的总概率，而总概率都是相同的，可以忽略**

#### 朴素贝叶斯假设
1. 设x(i)对于给定的条件y是独立的，这个假设叫做朴素贝叶斯假设，即一个词是在邮件中出现的事件和其他词是在邮件中出现事件是独立的。

>显然为假，所以称朴素贝叶斯

#### 注意：
xi....x3,x4,⋅xn，不同的邮件n是相同的，指的是字典单词的总个数。

## 
p(x∣y)指任意一个词在y=0，或y=1的出现的概率，结合垃圾邮件理解。


### 拉普拉斯平滑（Laplace smoothing）
目的：改进朴素贝叶斯算法的不足
朴素贝叶斯算法的不足：从未见过”nips“这个词，则模型认为该词在任何一类邮件中出现的概率皆为零。于是，当我们需要预测一封带有”nips“的邮件是否为垃圾邮件时，模型会计算其后验概率：
