# 斯坦福机器学习5



## 机器学习分类

有监督学习：输入有标签，从标记好的训练数据中进行训练，常用来做分类和回归。 

无监督学习：输入无标签，从未标记的训练数据中进行训练，根据数据的特征直接对数据的结构和数值进行归纳，常用来做聚类。 

半监督式学习：从部分标记的和部分没有标记的训练数据进行训练，常用来做回归和分类。



#强化学习

1. 强化学习中，我们只会给算法提供一个奖励函数（reword function），这个函数会告诉算法在什么情况下是做的好，在什么情况下是做的不好，比如在四腿机器人的例子中，当机器人向前行走时奖励函数会给算法正面的反馈，而当机器人无故后退或翻倒时函数则会给算法负面的反馈。而学习算法的任务就是自主发现“通过做出怎样的动作才能获得更多的奖励”。

2. 强化学习是一个过程

3. 强化学习的难题是要找到什么导致决策失败

 

##  马尔可夫模型

1. S是一个状态（states）的集合；（在直升机自动驾驶的例子中，就代表了直升机所有可能的位置和姿态。）

2. A是一个动作（action）的集合；（在直升机自动驾驶的例子中，就代表了通过操作杆可以对直升机做出的所有动作。）

3. Psa是状态转换概率，对每个状态s∈Ss∈S和动作a∈Aa∈A，PsaPsa是一个在状态空间上的分布。在后面我们会详细讨论这一点，简单的说，就是如果在状态ss下发生了动作aa，“那么下一步所将变为哪一个状态”的概率分布就由PsaPsa确定；

4. γ∈[0,1)称为折扣因子（discount factor），用来调整未来奖励与当作奖励之间的权重；

5. R:S×A→RR:S×A→R就是奖励函数。（有时只把奖励函数当做关于状态SS的函数R:S→R。）



>对于机器人，我们会在每一步加上一个小小的惩罚，这样使得机器可以更频繁移动，这样使得机器可以不浪费时间。（理解在其他不是目标的都有一定惩罚，使其快速找到目标）



>在得知最初奖励值（-1,+1）的情况下，得到11个方程，然后11个方程（按某种固定顺序/或者迭代），解出11个未知数，每一个方程就是0.8，0.1,0.1的概率，解出这11个方程，得到11个方向，整个图的方向确定，就是整个图的策略，而每一小格的方向就是那个小格的策略。

>递归迭代，默认结束条件就是目的地或者坑



[值迭代与策略迭代的区别](https://www.zhihu.com/question/41477987/answer/91389684)



## 马尔可夫模型的过程

(a) 在MDP中按照策略π执行一些试验；

(b) 使用在MDP的试验中积累的“经验”，更新对PsaPsa的估计（如果可以的话也更新R）；

(c) 使用估计的状态转换概率和奖励函数，应用值迭代，估计新的价值函数V；

(d) 使用关于V的贪心策略更新π

[课件](http://wenku.baidu.com/link?url=XW1LgXH3DdoHUnRof0st_DVof2c4o_5P80B55gaBbRw7pqt5NwIOxENh3YWHR-C8ODstXcnuJsXyUsRrXyE-itTIshiOx72WU7uCzA-RaES)





## 隐马尔可夫模型

[隐马尔可夫模型](https://www.zhihu.com/question/20962240)