# 斯坦福大学机器学习2

## 第六讲

## 文本分类的事件模型

### 多项事件模型
朴素贝叶斯算法算法的简单变形，称为多项事件模型
定义：p(xi∣y) ，这个式子和前面的完全一样，但是请注意，这里的特征值xi是邮件第i个单词，其取值不再是多元伯努利事件模型中的{0,1}了，它现在是一个多项分布。

#### 求参
最大化似然函数将得到各参数的最大似然估计

#### 注意：
xi....x3,x4,⋅xn，不同的邮件n是不同的，指的是邮件中单词的总个数。



#### 与朴素贝叶斯
1. 多项事件模型通常比原始的朴素贝叶斯算法效果更好，一个可能的原因是因为它考虑了每个单词出现的次数。
2. 尽管朴素贝叶斯分类器不是最好的分类算法，但它的效果一般都非常好，再加上它简单且易于实现的特性，我们通常用它作为“首选试验算法”。（使用朴素贝叶斯算法最终会得到一个逻辑函数形式的后验分布，也就是说，朴素贝叶斯算法也属于指数分布族，它仍然是一个线性分类器。）
>

# 支持向量机

>

### 线性分类器

## 基本概念

* 最大间隔分类器（maximum margin classifier，可以被看做是支持向量机的前身）实际上就选择特定的w,b使**几何间隔**最大化。
* 它通常被认为是最好的现成监督学习算法之一（很多人认为它是最好的）
* （找到第6讲笔记--间隔：直观概念）称这是一个良好的拟合，因为这反映出该拟合对分类结果的“信心”很足。所以，“信心”是一个很好的指标，后面我们将使用函数间隔形式化这个指标。考虑其空间意义，就是两类被模型描述相差越远（注意不是实际两类差别，因为两类差别是客观不变的，这里的差别指两类离这分界模型距离），那么模型就是最好的。在svm，用到的是间隔。

## 和Logistic回归的区别
**（详看第六讲讲义）**

*　就是在Z=w^T*x=>Z=w^T*x+b,加上b这个截距。是的y的值在-1和1之间
* 为了更加简洁的介绍支持向量机，我们需要先引入一种新的标记。考虑使用线性分类器解决“特征为x目标为y”的二元分类问题。这次，我们使用y∈{−1,1}来标记两个分类（而不是之前的y∈{0,1}），再使用参数向量w,b代替之前的参数向量θ，于是我们现在将分类器写为：hw,b(x)=g(wTx+b)，w是参数向量，b是一个实数

>**为什么直接用正负一？**



## 函数间隔及几何间隔

* 有hat的y就是函数间隔，否则为几何间隔。

###函数间隔

* 就是通过超平面原函数，当y(i)=1，w^T*x+b越大，且y(i)=0，w^T*x+b（为负数）越小来确定其间隔越大!

#### 引出||w||的理由
但是将(w,b)变为(2w,2b)相当于给函数间隔乘了系数2，于是我们发现，如果通过改变w,b的取值，我们可以让函数间隔变得很大，然而分类超平面并没有改变，所以单纯的通过这种方式改变函数间隔的大小没有什么实质意义。**直觉告诉我们**，应该引入一种标准化条件，比如令∥w∥=1（此时是垂直于超平面的单位向量）。


>我的理解，纯碎w，b改变，函数间隔改变了，但当纯碎w，b改变时，对于这里的g(z)来说，效果没有变化，故纯粹改变w，b时，不能用来作为确定更好参数的前提。函数间隔倍数增加无效，几何间隔加减变化有效，指倍数不能对正负改变但加减可以。
>>**记得看？**

#### 倍数增加无影响结果
* 有一个性质导致其函数间隔不能有效的反映预测的可信度：对于给定的g，如果将(w,b)替换为(2w,2b)，即g(wTx+b)变为g(2wTx+2b)，我们会发现分类超平面hw,b(x)并不会改变，也就是说hw,b(x)只关心wTx+b的正负，而不关心其大小。

>**为什么要成倍增加来看函数间隔**
>**答：因为这里是证明函数间隔不能用来作为依据，那么只要证明存在一种情况使得函数间隔变化结果却不变就可以了**


#### 设函数间隔为1的理由
按比例缩放参数(w,b)对假设结果没有任何影响，我们现在可以利用这一点。我们现在来引入限制条件：对于给定的训练集，以(w,b)为参数的函数间隔必须为1

>**为什么**
>**函数间隔和最优解无关，因为函数间隔可以任意变化，但问题还是原来的问题**

### 几何间隔
1. γ是为正值的时候，分类相应+1，否则-1。
2. 纯碎w，b改变，不改变几何间隔，因为有||w||的限制，因为超平面不会变，他们是等价的
3. 目的：给定一个x（i）得到它和超平面的距离

####理解间隔公式的入口
* 几何间隔实际上就是点到超平面的距离，高中时学过的点(x(i),y(i))到直线ax+by+c=0的距离，推广到高维就是上面的几何间隔，而函数间隔就是未标准化的几何间隔。**（在这里重点理解其公式，详看讲义：ch6最后）**

#### 两种间隔的关系：
* 就是几何间隔=函数间隔/||w||，简单来说就是函数间隔就是未标准化的几何间隔。（笔记写错）

### 我对函数间隔和几何间隔的理解
* 在理解函数间隔的时候，联想g(z)函数的样子，在理解几何间隔的时候，联想以w^T*x+b为超平面的距离计算公式的样子。在讲义上都可以看到。

#### 关于||w||

* ||W||：二范数,结果为w向量的各个元素的平方和的开平方,是线性代数的内容
* 在计算机领域，一般迭代前后步骤的差值的范数表示其大小，常用的是二范数，差值越小表示越逼近实际值，可以认为达到要求的精度，收敛。
* w^T*w =∥w∥^2
* 改变||w||对几何间隔没有影响
* 要带上限制条件∥w∥=1是因为我们的优化想要度量的是几何间隔

>反向传播：就是在神经网络中的最小二乘,神经网络可以得到非线性的分类边界

* 为参数的几何间隔γ定义为取所以独立的训练样本中最小的那个几何间隔（即取最坏的一个样本的情况）来表示超平面和训练数据的距离。

## 第七讲：

## 最优间隔分类器

目的：求出使得全局最小的参数

1. 对于给定的训练集，我们会自然的想要找到一个能够使（几何）间隔达到最大的判定边界，因为这样的判定边界能够得到高度可信的预测集，同时也是对训练集很好的拟合。这也将帮助我们得到一个将正负样本分隔（几何间隔）开的分类器。

> 不用函数间隔，

### 有关凸优化
* 简单的说，优化问题中，目标函数为凸函数，约束变量取值于一个凸集中的优化问题称为凸优化，举个简单例子，设S为凸集，f(x)为S上凸函数，则问题min f(x) s.t. x属于S。为一个凸优化。

* 设S为n维空间中的一个点集，X1、X2为S中的任两点。若对于任给的t，0<=t<=1，点X=tX1+(1-t)X2也属于S，则称S为n维空间中的一个凸集。组合tX1+(1-t)X2称为X1和X2的凸组合。简单的说，若两点在一个点集中，那么连接这两点的线段上所有点也在这个点集中，这样的点集就称为凸集。

**目标函数为凸函数，约束变量为凸集**

### 非凸性约束
* 这是一个糟糕的非凸性约束，即参数w可能位于一个单位圆环/球面上，不是凸集），这显然不是可以提交给标准的凸优化软件（如梯度下降法、牛顿法等）去解决的问题。

* 我们发现在第一个优化问题中，存在非凸性的限制条件，而在第二个优化问题中，存在非凸性的优化目标，所以我们并不能保证软件可以找到全局最小值--见讲义ch7


### 使其能够求解
* 按比例缩放参数(w,b)对假设结果没有任何影响，我们现在可以利用这一点。我们现在来引入限制条件：对于给定的训练集，以(w,b)为参数的函数间隔必须为1。

* 相当于优化目标可以类似用2次函数求解法来求解，也就是可以求解了

目的：得到最优间隔分类器的一般优化参数的公式

##拉格朗日乘数
拉格朗日乘数原理（即拉格朗日乘数法）由用来解决有约束极值的一种方法


## 引入对偶优化的原因
我们可以通过解对偶优化问题来得到原始优化问题的最优值（这么做的原因是，对偶问题通常更加简单，而且与原始问题相比，对偶问题具有更多有用的性质，稍后我们会在最优间隔分类器即支持向量机问题中见到），接下来看在什么情况下此式成立。


>**为什么maxL(w,α,β)在满足拉格朗日系数时=f（x）--ch7**
>**答：先把w看成常数，则maxL(w,α,β)时，f(x)就可以看做是一个常数，则常数的最大值就是他自己**


## 一些函数定义
1. 仿射函数/变换是指存在ai,bi使hi(w)->约束条件
2. 进一步假设gi是严格可用的，即对于所有i存在w能够使gi<=0，即式子有意义
3. 如果存在满足KKT条件的w∗,α∗,β∗，则原始问题与对偶问题一定有解，就是可以存在最优解
4. 即约束条件gi(w∗)≤0激活”，成为一个活动约束（active constraint）并处于取等号的状态，即g(w*)=0.

###仿射函数
h（i）是仿射函数（仿射函数/变换是指存在ai,bi使得hi(w)=aTiw+bi，而“仿射变换”是指线性变换后加上截距项bi使整体平移，即线性变换是固定原点的，而仿射变换是可以平移的

### KKT
* 当原始问题和对偶问题的最优值相等：时，可以用求解对偶问题来求解原始问题（当然是对偶问题求解比直接求解原始问题简单的情况下，在这里也要注意在支持向量机中应用了其性质），但是到底满足什么样的条件才能使的呢，这就是的 KKT条件

### 活动约束
就是：gi(w∗)=0
我们将通过这个条件知道支持向量机只有一小部分“支持向量”。当讲到序列最小优化算法（SMO）时，KKT对偶互补条件也会给我们一个验证其收敛特征的方法。

>**问：怎么知道函数间隔为1时，所对应的点就是支持向量，也就是计算函数间隔的点的？**
>**答：看《统计学习方法》p103，该例子中是先求出最优解，然后再求支持向量。反过来也成立**

###支持向量
也就是虚线上的三个点，只有这三个点的拉格朗日乘数不为零，也只有这三个样本的函数间隔等于1，其余样本的函数间隔都严格大于1。再多说一点，有时会有gi,αi都等于零的情况，但通常gi=0时αi是非零的，所以那些函数间隔为1的样本就是那些αi不等于零的样本），这三个样本也称为这个问题的支持向量（support vectors）。支持向量的数量通常都比训练集样本总量少很多。


### 我们使用拉格朗日对偶的原因
1. 我们可以通过解对偶优化问题来得到原始优化问题的最优值（这么做的原因是，对偶问题通常更加简单，而且与原始问题相比，对偶问题具有更多有用的性质，稍后我们会在最优间隔分类器即支持向量机问题中见到），接下来看在什么情况下此式成立。
2. 某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。
>用对偶很重要的原因在于：在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过求解对偶问题而得到原始问题的解，还有就是通过svm

### 求解的过程：
1. 首先用对偶性求出a，=>w用a表示=>是a,w带入原始问题求中解b

只有拉格朗日乘数a而没有beta_，因为此问题中只含有不等式约束条件。


由偶互补约束条件求出支持向量，先求出支持向量，=>求出用支持向量下的最大间隔，即最优解

假设我们已经通过拟合训练集得到模型的参数，现在想要预测一个新的输入x，那么我们会计算w^Tx+b(表示的就是z)，当且仅当这个值大于0时，模型才会给出结论y=1

**问：为什么在向量机中拉格朗日对偶比较好求**
**问：最大间隔的限制条件是怎么给出的**

# 第8讲

# 核方法
核方法的主要思想是基于这样一个假设：“在低维空间中不能线性分割的点集，通过转化为高维空间中的点集时，很有可能变为线性可分的” ，

## 低维转高维要解决的问题
一是由于是在高维度空间中计算，导致curse of dimension问题；二是非常的麻烦，每一个点都必须先转换到高维度空间，然后求取分割平面的参数等等

## 核方法和分类（或者回归）的问题
“为什么我们要关心向量的内积？”，一般地，我们可以把分类（或者回归）的问题分为两类：参数学习的形式和基于实例的学习形式。参数学习的形式就是通过一堆训练数据，把相应模型的参数给学习出来，然后训练数据就没有用了，对于新的数据，用学习出来的参数即可以得到相应的结论；而基于实例的学习（又叫基于内存的学习）则是在预测的时候也会使用训练数据，如KNN算法。而**基于实例的学习一般就需要判定两个点之间的相似程度**，一般就通过向量的内积来表达。从这里可以看出，核方法不是万能的，它一般只针对基于实例的学习。

## 核技巧
1. 讲输入空间映射到一个高维空间（一般是这样）
2. 通过內积求出相似性，这个和最大间隔求內积的道理是一样的，（內积的作用就是使求解变的简单）


**问：为什么核函数可以低维转高维**
**答：就是通过一个映射实现的，可能转化为维数极高的特征空间**

## 核方法的直观理解
能够直观（这种直观印象并非严格成立）的看出，如果向量ϕ(x)与ϕ(z)方向靠的比较近，那么根据K(x,z)=ϕ(x)Tϕ(z)可知这个值会比较大；反正，如果两个向量近乎正交（方向离的比较远），则K(x,z)=ϕ(x)Tϕ(z)将会很小。如此，我们就可以用K(x,z)度量ϕ(x)与ϕ(z)的相似度，或者x与z的相似度。

### 核矩阵
我们已经有了一个有效核K，现在我们有一个有限集合{x(1),⋯,x(m)}（不一定是训练集，可以是任意一个包含m个点的集合），含有m个点，然后在构造一个m阶方阵K，它的第(i,j)个元素定义为Kij=K(x(i),x(j))。

##半正定矩阵
* 实对称矩阵：如果有n阶矩阵A，其各个元素都为实数，矩阵A的转置等于其本身，则称A为实对称矩阵。
* 半正定矩阵：设A是实对称矩阵。如果对任意的实非零列矩阵X有XT*A*X≥0，就称A为半正定矩阵。

## Mercer 定理：
任何半正定的函数都可以作为核函数。所谓半正定的函数f(xi,xj)，是指拥有训练数据集合（x1,x2,...xn)，我们定义一个矩阵的元素aij = f(xi,xj)，这个矩阵式n*n的，如果这个矩阵是半正定的，那么f(xi,xj)就称为半正定的函数。（这个mercer定理不是核函数必要条件，只是一个充分条件，即还有不满足mercer定理的函数也可以是核函数。）--这个有待确定。常见的核函数有高斯核，多项式核等等，在这些常见核的基础上，通过核函数的性质（如对称性等）可以进一步构造出新的核函数。

##
先默认內积为降为关键，

### 正则化

，我们都是在数据线性可分隔的假设下讨论支持向量机算法的。通过ϕϕ将数据映射到更高纬度的特征空间通常能够增加数据可分隔的概率，但是我们并不能保证总是如此。受可能存在的离群值的影响，在某些情形下，我们并不能直接用算法确定分类超平面。比如下图的情形：
为了使算法在面对线性不可分隔的数据集时有效，同时也为了降低算法对离群值的敏感度，我们重新定义了如下的优化目标

>就是把最小间隔加上一个常数，这个时候并不是求的最小的那个间隔作为支持向量，而是加上正则惩罚项后的

## 序列最小优化算法
最小就是指一次改变最少个αi，在这里即两个。
目的：求解最优化的a1的值

### 二次函数的smo
![](http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/resource/chapter08_image03.png)

### 二元偏导数的几何意义

![](http://h.hiphotos.baidu.com/zhidao/wh%3D600%2C800/sign=b1b9945ab8389b5038aae854b505c9e5/0df3d7ca7bcb0a467577057d6a63f6246b60af19.jpg)
比如一个椭球面,它有无数个点,有其中一点（a,b,c） 函数对x的偏导数 就是 阴影椭圆形的线框（平行于x0z面）,再建立坐标x‘0z’,仅考虑该坐标的话  有函数z=f（x）  f'（x）是z=f（x）的导数（也是斜率）同时也等于球面函数在（a,b,c）点对于x的偏导


**问：怎么理解坐标下降/上升法？**
**答：看上图，一开始我们确定(a,b,c然后对x求偏导数，这个时候y是固定的，然后再对y求偏导数，这个时候x是固定的)**

>[坐标上升](http://blog.csdn.net/google19890102/article/details/51065297)，这个博客唯一可能会误导的是，首先要确定初始的点，而博客中没有

### 梯度下降和坐标下降的区别
* 梯度下降法又称为最速下降法，他也是下降法，不过和坐标下降法的主要区别就是多了一个下降方向的选取，在坐标下降中下降方向是沿着每一维的坐标轴方向进行的，也就是方向是类似于（0,0,1,0,0）、（0,0,0,1,0）（假设是5维）这种形式的，而梯度下降法中，下降方向变换为函数在当前点的梯度方向，当维度很高时，梯度下降的优势就要比坐标下降明显很多。



每次迭代a是w(a)总值只会越小，也就是越接近最优解，或者至少w(a)没有改变。






