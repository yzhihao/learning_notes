# 降维技术

# 第13讲

1. 混合高斯上的分布看讲义（高斯分布和多项式分布）
2. 然后就是令导数等于0，求出参数




## 因子分析
* n≫m（特征远大于样本的数）时。在这种情形下，因为样本太少，即使只用一个高斯分布也难以对数据进行建模，更别说用多个高斯分布将数据分开了。特别是因为样本短缺，仅mm个样本在RnRn空间中只能张成一个维度很低的子空间，如果此时使用高斯分布建模，并使用最大似然估计对期望和协方差做出估计时

### 用它的原因：
1. 常用
2. 简单，不用太多的数学公式

只有两个数据的时候时，计算其协方差就会发现得到的是不可逆的一个矩阵
**非奇异的协方差矩阵会导致，行列式等于0，也就导致极大似然1/0产生，这不是一个好的现象**

## 对称矩阵的意义
1. 高斯分布的主轴平行于坐标，也就是此时协方差矩阵非对角线全为0，理解椭圆的平行于坐标
2. 2个数据也可以得到非奇异阵，不过会失去原来的性质

## 更严格的规范-对角阵，且值相同
在这种情形下的高斯分布概率密度的等高线图将是正圆形（在二维情况下，而非普通ΣΣ下的椭圆；在高维情况下则是球面或超球面

### 引出因子分析
将Σ限制为对角矩阵也就意味着样本的任意两个分量xi,xj之间是无关且独立的。通常，在不加约束条件时，通过数据拟合得到的Σ将捕获到一些关于数据的有趣的潜在联系，而我们使用上面任意一种约束后，就相当于强行抹掉了样本各分量之间的联系。在后面对因子分析模型的介绍中，我们将在对角矩阵约束之外添加更多参数，用以捕获数据间的联系，但同样不会拟合一个完整的协方差矩阵。
>由此得知因子分析的目的：添加约束抹掉联系，分析其丢失的联系，得到样本之间的联系

### 协方差矩阵
如果样本数量mm没有比特征数量nn大出合理的倍数，则得到的期望和协方差参数都会很差。尽管如此，我们依然希望使用一个合理的高斯模型拟合数据集，也许能够得到一些有趣的关于数据的协方差矩阵，

>如果我们要用数据集拟合一个完整的、不加约束条件的协方差矩阵Σ，则至少要满足m≥n+1，以保证Σ的最大似然估计是非奇异矩阵。在上面的两种情形下，只要m≥2，我们得到的Σ就是非奇异的。**这里的约束条件是：对角矩阵，其对角线元素是相等**


1. 对分问题有什么的帮助：等高线图将是正圆形
2. 高斯分布的边缘分布及条件分布是怎么来的？
3. 为什么可以通过边缘分布来得到参数？
4. 边缘分布和p(x|θ的关系)
5. 协方差是怎么决定可以拟合样本较小的数据的


### 高斯分布的边缘分布及条件分布



### 因子分析模型
[因子分析 ](http://blog.csdn.net/littleqqqqq/article/details/50899717)

[因子分析和主成分分析的差别](http://www.cnblogs.com/appboling/p/3797151.html)

## 因子分析
因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示。

### PCA算法，不过通常在PCA算法运行前，我们需要对数据进行预处理——对数据的期望及方差进行标准化

1. 整体的数据的期望设为0
2. 各个分量用统一的标准定义，（如一个人的身高，体重...）

### 我们的目标
1. 使得原始点投影到上的点具有较小的方差。也就是靠的很近,这样会使得（比如离化为的一维直线越远）
2. 也就是说：主成份分析目的就是使得数据点在特定的向量上的投影方差最大。而我们就是要求出这个向量。


### 特征向量和主特征向量
1. aV=VM：V叫做M的特征向量，a为M的特征值。aV是V的一个缩放版本。无论M怎样与任意向量相乘都只是在缩放特征向量而已。
2. 有最大特征值对于的特征向量就是主特征向量
3. 在主成分分析中,主特征向量就是那个低维（直线）的单位向量
[特征向量](http://www.cnblogs.com/chaosimple/p/3179695.html)

## SVD
* 另一种实现PCA的方法。
* 使用SVD实现PCA算法的计算过程比用特征向量快很多。

### 文本分析
如果文档jj包含单词“study”而文档jj包含单词“learn”，那么如果一篇介绍“study strategy”的文章和一篇介绍“method of learning”的文章在这种算法下就是无关的，我们现在想要使它们相关。于是一开始，“learn”向量与“study”向量是相互正交的，它们的内积为零，我们在这两个向量之间再找一个向量uu，然后将“learn”与“study”投影在uu上，此时两个向量的投影点在uu上将相距很近。

**问：是否没**

## ICA
尽管ICA对于服从高斯分布的数据存在这样的缺陷，但只要数据不服从高斯分布，在数据充足的情况下，我们还是可以分离出n个独立的源的


