# 斯坦福机器学习4

# 无监督学习

# 第12讲

## k-means算法



### k-means算法是否一定收敛
*　k -means算法一定收敛吗？在特定的场景下，答案是肯定的。
*　可以证明k-means正是在J上的坐标下降过程（参考第八讲）。尤其是k-means算法的内循环实际上是在重复：保持μ不变最小化J关于c的函数，然后再保持c不变最小化J关于μ的函数。因此，J是单调递减的，它的函数值一定收敛


### k-means算法的凸性质
* 失真函数J是一个非凸函数，所以对于JJ的坐标下降并不能保证其收敛于全局最小值，即kk-means会被局部最小值影响。尽管如此，kk-means算法通常都都能很好的完成聚类任务。
>也就是说初始值对结果的影响较大
>在聚类的时候要多选几次随机初始化，然后选择最小



### 检测异常
* 若P(x)是符合一个分布，则当新的x使得p(x)的值很小的时候，那么这个值就是异常的。
* 联系

### 关于z（i）
变量z(i)类似于样本x(i)x(i)的标记，只是我们并不知道这个变量是多少，也就是说我们不知道每个样本属于哪个标记类，在后面的算法中，我们会尝试猜测这个标记

### 混合高斯模型的求解
[混合高斯模型-matlab](http://blog.csdn.net/abcjennifer/article/details/8198352)
[混合高斯模型-python](http://blog.csdn.net/golden1314521/article/details/46051431#comments)
高斯判别分析模型中的参数估计几乎是一样的，只不过z(i)z(i)代替了原来的类标记y(i)y(i)。（式子里还有其它细节不同，在问题集1的高斯判别分析中我们一了解到：第一，不同与问题集中y(i)∈{−1,1}y(i)∈{−1,1}服从伯努利分布，我们已经将z(i)z(i)泛化为多项分布；第二，不同于问题集中不同的yy共用一个协方差矩阵，我们已经给每个高斯分布赋上了不同的协方差矩阵ΣjΣj。

## EM算法
1. 为了求对于p(x|θ)的极大似然，也就是为了求p(x,z|θ)
2. 化为EM算法之后，就是得到一指数族的分布，想混合高斯分布，这样求解变的很好求


### em算法的另一种理解
就是



求出Q函数，代表的是未观测数据的在观测数据和参数下的概率分布的期望
迭代求出使Q函数极大化的参数θ
最终目的就是为了求出极大化观测数据的关于θ的极大似然函数
上面一句话中观测数据的关于θ的极大似然函数相当于未观测数据的在观测数据和参数下的概率分布的期望（通过jensen不等式得到）Q函数
通过求下界的最大值，改变θ，在将θ代入求Q函数得到新的Q函数，再迭代下一步到求Q函数的极大化值

>在E步和M步骤之间使用jensen不等式得到下界函数，并在M步骤求出下界函数的极大值，也就是在<统计学习方法>p159的B函数，因为这个B函数式可求的，求这个B函数的极大值，更新θ。
>**当然B和讲义上的最后函数的意义是一样的，只不过推导的过程不一样。**
>E


确定Q函数，最大化似然Q函数，就相当于最大化已观测数据的最大化==全数据的最大化似然函数，相当于创造y值，然后分类

### 延森不等式
**目的：就是得到一次迭代的下界，然后为了是增大下界然后让原函数的值变得更大，更接近最大值**

### 注意
1. 参数初始值比较敏感，而且不能保证收敛到全局最优


**目的：EM算法正是服务于求解带有隐变量的参数估计问题**
是一种迭代算法，有两个主要步骤。对于上面的问题，E步骤中，算法尝试猜测每个z(i)z(i)的值，在M步骤中算法会根据猜测更新模型。因为M步骤会假设E步骤中的猜测是正确的，那么，有了缺失的z(i)z(i)，最大化就很简单了

###E步骤:
算出z(i)在第j个高斯分布的概率
**E步骤怎么理解？**

## z(i)的猜测
软”意味着一种概率上的猜测，从[0,1][0,1]区间取值；相对的“硬”猜测意味着最佳的一次性猜测，是一种对于值的猜测，比如从集合{0,1}{0,1}或{1,⋯,k}{1,⋯,k}中取值

