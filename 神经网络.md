# 神经网络

## 参数维数
* If network has sj units in layer j and sj+1 units in layer j+1, then Θ(j) will be of dimension sj+1×(sj+1).
![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0rgjYLDeEeajLxLfjQiSjg_0c07c56839f8d6e8d7b0d09acedc88fd_Screenshot-2016-11-22-10.08.51.png?expiry=1483228800000&hmac=wPc0KITCl2NRui_LFLwpM6HmqvaSveibYxdDdu9vqbg)
详情看：[Model Representatio](https://www.coursera.org/learn/machine-learning/supplement/Bln5m/model-representation-i)
>要会计算维数

## 初始化参数
* 在logistics回归中可以初始化为0，单神经网络中不可以


### Backpropagation Algorithm
* compute partial derivatives
[课件](https://www.coursera.org/learn/machine-learning/supplement/pjdBA/backpropagation-algorithm)


### Gradient Checking
* Once you have verified once that your backpropagation algorithm is correct, you don't need to compute gradApprox again. The code to compute gradApprox can be very slow.
* that why we shoulf use Backpropagation Algorithm but not forward propagation  to compute partial derivatives
[Gradient Checking](https://www.coursera.org/learn/machine-learning/supplement/fqeMw/gradient-checking)

## 步骤
1. Randomly initialize the weights
2. Implement forward propagation to get hΘ(x(i)) for any x(i)
3. Implement the cost function
4. Implement backpropagation to compute partial derivatives
5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.
6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.